
networks:
  shared-network:
    driver: bridge
services:
  spark-master:
    image: bitnami/spark:latest
    container_name: spark-master
    labels:
      - "role=master"
    user: root
    ports:
      - "8082:8080"
      - "7077:7077"
    environment:
      - SPARK_MODE=${SPARK_MODE_MASTER}
      - HADOOP_USER_NAME=${HADOOP_USER_NAME}
      - SPARK_USER=${SPARK_USER}
    networks:
      - shared-network
    volumes:
      - ./config/spark-defaults.conf:/opt/spark/conf/spark-defaults.conf
      - ./config/core-site.xml:/opt/bitnami/spark/conf/core-site.xml
      - ./config/ivysettings.xml:/opt/bitnami/spark/conf/ivysettings.xml
      - ./examples/spark-iceberg:/opt/bitnami/spark/jobs
      - ./data:/opt/bitnami/spark/data
      - ./drivers:/opt/bitnami/spark/external-jars
      - ./iceberg-warehouse:/opt/iceberg/warehouse  # Iceberg 仓库

  spark-worker:
    image: bitnami/spark:latest
    container_name: spark-worker
    labels:
      - "role=worker"
    user: root
    depends_on:
      - spark-master
    environment:
      - SPARK_MODE=${SPARK_MODE_WORKER}
      - SPARK_MASTER_URL=${SPARK_MASTER_URL}
      - HADOOP_USER_NAME=${HADOOP_USER_NAME}
      - SPARK_USER=${SPARK_USER}
    networks:
      - shared-network
    volumes:
      - ./config/core-site.xml:/opt/bitnami/spark/conf/core-site.xml
      - ./config/ivysettings.xml:/opt/bitnami/spark/conf/ivysettings.xml
      - ./data:/opt/bitnami/spark/data
    ports:
      - "8083:8083"

  kafka:
    image: bitnami/kafka:latest
    container_name: kafka-container
    ports:
      - "9092:9092"
    environment:
      # KRaft mode configuration
      - KAFKA_CFG_NODE_ID=${KAFKA_CFG_NODE_ID}
      - KAFKA_CFG_PROCESS_ROLES=${KAFKA_CFG_PROCESS_ROLES}
      - KAFKA_CFG_CONTROLLER_QUORUM_VOTERS=${KAFKA_CFG_CONTROLLER_QUORUM_VOTERS}
      - KAFKA_CFG_LISTENERS=${KAFKA_CFG_LISTENERS}
      - KAFKA_CFG_ADVERTISED_LISTENERS=${KAFKA_CFG_ADVERTISED_LISTENERS}
      - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=${KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP}
      - KAFKA_CFG_CONTROLLER_LISTENER_NAMES=${KAFKA_CFG_CONTROLLER_LISTENER_NAMES}
      - KAFKA_CFG_INTER_BROKER_LISTENER_NAME=${KAFKA_CFG_INTER_BROKER_LISTENER_NAME}
    networks:
      - shared-network

  jobmanager:
    image: flink:latest
    container_name: flink-jobmanager
    hostname: jobmanager
    networks:
      - shared-network
    ports:
      - "8081:8081"
    command: jobmanager
    environment:
      - FLINK_PROPERTIES=${FLINK_PROPERTIES}
    volumes:
      - ./examples/flink-iceberg:/opt/flink/examples
      - ./iceberg-warehouse:/opt/iceberg/warehouse
      - flink-checkpoints:/opt/flink/checkpoints
      - flink-savepoints:/opt/flink/savepoints
    depends_on:
      - kafka
      - nessie
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8081"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s

  taskmanager:
    image: flink:latest
    container_name: flink-taskmanager
    networks:
      - shared-network
    command: taskmanager
    environment:
      - FLINK_PROPERTIES=${FLINK_PROPERTIES}
    volumes:
      - ./iceberg-warehouse:/opt/iceberg/warehouse
      - flink-checkpoints:/opt/flink/checkpoints
      - flink-savepoints:/opt/flink/savepoints
    depends_on:
      - jobmanager
    healthcheck:
      test: ["CMD-SHELL", "ps aux | grep -v grep | grep taskmanager"]
      interval: 30s
      timeout: 10s
      retries: 5

  minio:
    image: minio/minio
    container_name: minio
    networks:
      - shared-network
    ports:
      - "9000:9000"
      - "9001:9001"
    command: server /data --console-address ":9001"
    environment:
      - MINIO_ACCESS_KEY=${MINIO_ACCESS_KEY}
      - MINIO_SECRET_KEY=${MINIO_SECRET_KEY}
    volumes:
      - minio-data:/data

  trino:
    image: trinodb/trino:latest
    container_name: trino
    networks:
      - shared-network
    ports:
      - "8084:8080"
    volumes:
      - ./config/trino:/etc/trino
      - ./iceberg-warehouse:/opt/iceberg/warehouse
    environment:
      - TRINO_ENVIRONMENT=${TRINO_ENVIRONMENT}
      - AWS_ACCESS_KEY_ID=${MINIO_ACCESS_KEY}
      - AWS_SECRET_ACCESS_KEY=${MINIO_SECRET_KEY}
      - AWS_ENDPOINT_URL=http://minio:9000
      - AWS_REGION=us-east-1
    depends_on:
      - nessie
      - minio

  nessie:
    image: projectnessie/nessie
    container_name: nessie
    networks:
      - shared-network
    ports:
      - "19120:19120"
    environment:
      - QUARKUS_DATASOURCE_JDBC_URL=${QUARKUS_DATASOURCE_JDBC_URL}
      - QUARKUS_DATASOURCE_USERNAME=${QUARKUS_DATASOURCE_USERNAME}
      - QUARKUS_DATASOURCE_PASSWORD=${QUARKUS_DATASOURCE_PASSWORD}
      - NESSIE_VERSION_STORE_TYPE=${NESSIE_VERSION_STORE_TYPE}
    depends_on:
      - postgres
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:19120/api/v1/trees"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s

  postgres:
    image: postgres:15
    container_name: postgres
    networks:
      - shared-network
    ports:
      - "5432:5432"
    environment:
      - POSTGRES_DB=${POSTGRES_DB}
      - POSTGRES_USER=${POSTGRES_USER}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
    volumes:
      - postgres-data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U nessie"]
      interval: 10s
      timeout: 5s
      retries: 5

  dremio:
    image: dremio/dremio-oss
    container_name: dremio
    networks:
      - shared-network
    ports:
      - "9047:9047"   # UI
      - "31010:31010" # JDBC
      - "45678:45678" # Internal
    volumes:
      - dremio-data:/opt/dremio/data
      - ./iceberg-warehouse:/opt/iceberg/warehouse:rw  # Iceberg warehouse (read-write)
      - ./data:/opt/dremio/datasets:rw                 # Sample datasets
    environment:
      - DREMIO_JAVA_SERVER_EXTRA_OPTS=${DREMIO_JAVA_SERVER_EXTRA_OPTS}
      - DREMIO_MAX_HEAP_MEMORY_SIZE_MB=${DREMIO_MAX_HEAP_MEMORY_SIZE_MB}
      - DREMIO_MAX_DIRECT_MEMORY_SIZE_MB=${DREMIO_MAX_DIRECT_MEMORY_SIZE_MB}
    depends_on:
      - minio
      - nessie
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9047/status"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s

volumes:
  minio-data:
  flink-checkpoints:
  flink-savepoints:
  postgres-data:
  dremio-data:
